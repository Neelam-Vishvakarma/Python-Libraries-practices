{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Notebook\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import PySpark and create SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder.appName(\"PySparkBasics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a sample DataFrame manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "java.lang.IllegalArgumentException: Error while instantiating 'org.apache.hadoop.hive.ql.exec.FunctionRegistry'.",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# Create some sample data as list of tuples\n",
    "data = [\n",
    "    (1, \"Alice\", 29, \"HR\"),\n",
    "    (2, \"Bob\", 31, \"Engineering\"),\n",
    "    (3, \"Charlie\", 25, \"Marketing\"),\n",
    "    (4, \"David\", 35, \"Engineering\"),\n",
    "    (5, \"Eve\", 28, \"HR\")\n",
    "]\n",
    "\n",
    "# Define schema/column names\n",
    "columns = [\"ID\", \"Name\", \"Age\", \"Department\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Basic DataFrame operations\n",
    "- Show schema\n",
    "- Select columns\n",
    "- Filter rows\n",
    "- Add new column\n",
    "- Drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Department: string (nullable = true)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+-----+-------+\n",
       "| Name|    Age|\n",
       "+-----+-------+\n",
       "|Alice|     29|\n",
       "|  Bob|     31|\n",
       "|Charlie|   25|\n",
       "| David|     35|\n",
       "|  Eve|     28|\n",
       "+-----+-------+"
      ]
     },
     "execution_count": 3
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+---+-------+---+-----------+\n",
       "| ID|   Name|Age| Department|\n",
       "+---+-------+---+-----------+\n",
       "|  2|    Bob| 31|Engineering|\n",
       "|  4|  David| 35|Engineering|\n",
       "+---+-------+---+-----------+"
      ]
     },
     "execution_count": 3
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+---+-------+---+-----------+----------+\n",
       "| ID|   Name|Age| Department|SalaryUSD|\n",
       "+---+-------+---+-----------+----------+\n",
       "|  1|  Alice| 29|         HR|    60000|\n",
       "|  2|    Bob| 31|Engineering|    70000|\n",
       "|  3|Charlie| 25|  Marketing|    55000|\n",
       "|  4|  David| 35|Engineering|    72000|\n",
       "|  5|    Eve| 28|         HR|    61000|\n",
       "+---+-------+---+-----------+----------+"
      ]
     },
     "execution_count": 3
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+---+-------+---+\n",
       "| ID|   Name|Age|\n",
       "+---+-------+---+\n",
       "|  1|  Alice| 29|\n",
       "|  2|    Bob| 31|\n",
       "|  3|Charlie| 25|\n",
       "|  4|  David| 35|\n",
       "|  5|    Eve| 28|\n",
       "+---+-------+---+"
      ]
     },
     "execution_count": 3
    }
   ],
   "source": [
    "# Show DataFrame schema\n",
    "df.printSchema()\n",
    "\n",
    "# Select Name and Age columns\n",
    "df.select(\"Name\", \"Age\").show()\n",
    "\n",
    "# Filter rows where Department is Engineering\n",
    "df.filter(df.Department == \"Engineering\").show()\n",
    "\n",
    "# Add new column SalaryUSD\n",
    "df = df.withColumn(\"SalaryUSD\", df.Age * 2000 + 10000)\n",
    "df.show()\n",
    "\n",
    "# Drop Department column\n",
    "df = df.drop(\"Department\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Grouping and Aggregations\n",
    "- Group by Department and calculate average Age (recreate DataFrame with Department first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "| Department|AvgAge  |\n",
      "+-----------+--------+\n",
      "|         HR| 28.5   |\n",
      "|Engineering| 33.0   |\n",
      "|  Marketing| 25.0   |\n",
      "+-----------+--------+\n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "# Re-create original df with Department column\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Group by Department and calculate average Age\n",
    "df.groupBy(\"Department\").avg(\"Age\")\n",
    "  .withColumnRenamed(\"avg(Age)\", \"AvgAge\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sorting\n",
    "- Sort by Age descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+\n",
      "| ID|   Name|Age| Department|\n",
      "+---+-------+---+-----------+\n",
      "|  4|  David| 35|Engineering|\n",
      "|  2|    Bob| 31|Engineering|\n",
      "|  1|  Alice| 29|         HR|\n",
      "|  5|    Eve| 28|         HR|\n",
      "|  3|Charlie| 25|  Marketing|\n",
      "+---+-------+---+-----------+\n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "df.orderBy(df.Age.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save DataFrame to CSV (local filesystem)\n",
    "\n",
    "*Note:* On some platforms, CSV may be saved as multiple part-files in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Data saved to output/employees.csv\n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "output_path = \"output/employees.csv\"\n",
    "df.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Stop SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
